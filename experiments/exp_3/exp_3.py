"""A trained Long-Short Term Memory RNN model is used to generate the next K notes of a melody given a sequence of them mapped linearly from Hue values of pixels. The music is based on the 6 suites of cello written by J. S. Bach."""
# The code model was modified from a version of the code of the Generative Deep Learning book
# by David Foster, you can find it here: https://github.com/davidADSP/GDL_code
import ast
import os
import pickle as pkl
from datetime import datetime
from pathlib import Path
from typing import Any
from functools import lru_cache

import cv2
import numpy as np
import pandas as pd
from music21 import instrument, note, stream, chord, duration

from experiments.exp_3.lstm_model import create_network, sample_with_temp
from image_to_melody.audio_processor import synthesize
from image_to_melody.connections import download_sound_font
from image_to_melody.img_utils import get_representative_pixels, HUE_MAX_VAL
from image_to_melody.utils import map_value_to_dest


here = Path(__file__).resolve().parent


class Conf:
    # General
    EXPERIMENT_ID = 3
    # IMAGE PROCESSING
    NUMBER_SLICES = 5
    K = 5 # number representative pixels to get from each slice
    # MELODY GENERATION
    SAMPLE_RATE = 44100 # 44.1 KHz - standard used in most CDs and digital audio format
    # AUDIO POST-PROCESSING
    TEMP_MIDI_FILEPATH = "experiments/tmp/melody_{date_}.mid"
    TEMP_AUDIO_FILEPATH = "experiments/tmp/melody_{date_}.wav"
    SOUND_EFFECTS = [] # Pedalboard effects
    SOUND_FONT_FILEPATH = "experiments/tmp/sound_font.sf2"
    # VIDEO GENERATION
    RATE_IMG_REPETITION = 5
    FPS = 5
    # DEEP LEARNING MODEL
    ARTIFACTS_FOLDER = here / "artifacts"
    USE_ATTENTION = True
    EMBEDDING_SIZE = 100
    RNN_UNITS = 256
    NOTES_TEMPERATURE = 0.5
    DURATION_TEMPERATURE = 0.5
    MAX_EXTRA_NOTES = 50
    MAX_SEQ_LEN = NUMBER_SLICES * K
    SEQ_LEN = NUMBER_SLICES * K


@lru_cache(maxsize=10)
def load_single_notes_set(path: str) -> list:
    """Read txt with single notes stored as a list."""
    with open(path, "r") as f:
        return ast.literal_eval(f.read())


@lru_cache(maxsize=10)
def load_model_metadata(path: str):
    """Load model's metadata saved as pickle file."""
    with open(path, "rb") as f:
        return pkl.load(f)


def generate_notes(
    model: Any,
    notes: list,
    durations: list,
    note_to_int: dict,
    duration_to_int: dict,
    int_to_note: dict,
    int_to_duration: dict,
):
    prediction_output = []
    notes_input_sequence = []
    durations_input_sequence = []

    overall_preds = []

    for n, d in zip(notes, durations):
        note_int = note_to_int[n]
        duration_int = duration_to_int[d]
        
        notes_input_sequence.append(note_int)
        durations_input_sequence.append(duration_int)

        if n != "START":
            midi_note = note.Note(n)

            new_note = np.zeros(128)
            new_note[midi_note.pitch.midi] = 1

    for note_index in range(Conf.MAX_EXTRA_NOTES):
        prediction_input = [
            np.array([notes_input_sequence]), np.array([durations_input_sequence])
        ]

        notes_prediction, durations_prediction = model.predict(prediction_input, verbose=0)
        
        new_note = np.zeros(128)
        
        for idx, n_i in enumerate(notes_prediction[0]):
            try:
                note_name = int_to_note[idx]
                midi_note = note.Note(note_name)
                new_note[midi_note.pitch.midi] = n_i
            except:
                pass
            
        overall_preds.append(new_note)
        
        i1 = sample_with_temp(notes_prediction[0], Conf.NOTES_TEMPERATURE)
        i2 = sample_with_temp(durations_prediction[0], Conf.DURATION_TEMPERATURE)

        note_result = int_to_note[i1]
        duration_result = int_to_duration[i2]
        
        prediction_output.append([note_result, duration_result])

        notes_input_sequence.append(i1)
        durations_input_sequence.append(i2)
        
        if len(notes_input_sequence) > Conf.MAX_SEQ_LEN:
            notes_input_sequence = notes_input_sequence[1:]
            durations_input_sequence = durations_input_sequence[1:]

    overall_preds = np.transpose(np.array(overall_preds)) 
    print('Generated sequence of {} notes'.format(len(prediction_output)))

    return prediction_output


def generate_midi_file(notes_and_durations: list, output_path: str):
    midi_stream = stream.Stream()

    # create note and chord objects based on the values generated by the model
    for pattern in notes_and_durations:
        note_pattern, duration_pattern = pattern
        # pattern is a chord
        if ("." in note_pattern):
            notes_in_chord = note_pattern.split(".")
            chord_notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(current_note)
                new_note.duration = duration.Duration(duration_pattern)
                new_note.storedInstrument = instrument.Violoncello()
                chord_notes.append(new_note)
            new_chord = chord.Chord(chord_notes)
            midi_stream.append(new_chord)
        elif note_pattern == "rest":
            # pattern is a rest
            new_note = note.Rest()
            new_note.duration = duration.Duration(duration_pattern)
            new_note.storedInstrument = instrument.Violoncello()
            midi_stream.append(new_note)
        elif note_pattern != "START":
            # pattern is a note
            new_note = note.Note(note_pattern)
            new_note.duration = duration.Duration(duration_pattern)
            new_note.storedInstrument = instrument.Violoncello()
            midi_stream.append(new_note)

    midi_stream.write("midi", fp=output_path)


def image_to_melody(image: np.ndarray):
    hsv_img = cv2.cvtColor(src=image, code=cv2.COLOR_BGR2HSV)

    all_representative_pixels = get_representative_pixels(
        hsv_img, Conf.NUMBER_SLICES, Conf.K
    )

    df_repixels = pd.DataFrame(
        all_representative_pixels, columns=["hue", "saturation", "value"]
    )

    # Load model's metadata
    single_notes_set = load_single_notes_set(
        path=os.path.join(Conf.ARTIFACTS_FOLDER, "single_notes_set.txt")
    )

    note_names, n_notes, duration_names, n_durations = load_model_metadata(
        path=os.path.join(Conf.ARTIFACTS_FOLDER, "distincts.pkl")
    )

    note_to_int, int_to_note, duration_to_int, int_to_duration = load_model_metadata(
        path=os.path.join(Conf.ARTIFACTS_FOLDER, "lookups.pkl")
    )

    # Generate notes using Hue pixel's value
    step = HUE_MAX_VAL / len(single_notes_set)
    thresholds = np.linspace(start=step, stop=HUE_MAX_VAL, num=len(single_notes_set))

    df_repixels["notes"] = df_repixels.apply(
        lambda row : map_value_to_dest(row["hue"], single_notes_set, thresholds=thresholds), axis=1,
    )

    # Generate NUMBER_SLICES * K notes
    model, _ = create_network(
        n_notes, n_durations, Conf.EMBEDDING_SIZE, Conf.RNN_UNITS, Conf.USE_ATTENTION
    )

    model.load_weights(os.path.join(Conf.ARTIFACTS_FOLDER, "weights.h5"))

    notes = list()
    durations = list()

    for _, row in df_repixels.iterrows():
        notes.append(row["notes"])
        durations.append(1)

    notes[0] = "START"
    durations[0] = 0

    print(notes)

    notes_and_durations_output = generate_notes(
        model,
        notes,
        durations,
        note_to_int,
        duration_to_int,
        int_to_note,
        int_to_duration,
    )

    # Generate audio
    date_ = datetime.now().strftime("%Y%m%d_%H%M%S")

    midi_path = Conf.TEMP_MIDI_FILEPATH.format(date_=date_)
    generate_midi_file(
        notes_and_durations_output,
        output_path=midi_path,
    )

    download_sound_font(Conf.SOUND_FONT_FILEPATH)
    audio_output_path = Conf.TEMP_AUDIO_FILEPATH.format(date_=date_)
    synthesize(
        midi_file_path=midi_path,
        output_path=audio_output_path,
        sample_rate=Conf.SAMPLE_RATE,
        sound_font_filepath=Conf.SOUND_FONT_FILEPATH,
    )

    try:
        os.remove(midi_path)
    except:
        print("Warning. MIDI file wasn't deleted")

    return audio_output_path
